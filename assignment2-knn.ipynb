{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import ctime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start program time at Sat Jun  2 17:59:59 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0 fold at  Sat Jun  2 18:00:04 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.72      0.33      0.45     21184\n",
      "          2       0.61      0.78      0.68     28331\n",
      "          3       0.94      0.84      0.88      3576\n",
      "          4       0.78      0.67      0.72       275\n",
      "          5       0.17      0.84      0.29       950\n",
      "          6       0.78      0.86      0.82      1737\n",
      "          7       0.91      0.95      0.93      2051\n",
      "\n",
      "avg / total       0.68      0.63      0.62     58104\n",
      "\n",
      "end  0 fold at Sat Jun  2 18:00:19 2018\n",
      "\n",
      "\n",
      "start  1 fold at  Sat Jun  2 18:00:19 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.45      0.36      0.40     21184\n",
      "          2       0.58      0.69      0.63     28330\n",
      "          3       0.97      0.77      0.86      3576\n",
      "          4       0.72      0.83      0.77       275\n",
      "          5       0.50      0.80      0.62       950\n",
      "          6       0.64      0.73      0.68      1737\n",
      "          7       0.97      0.57      0.72      2051\n",
      "\n",
      "avg / total       0.57      0.57      0.56     58103\n",
      "\n",
      "end  1 fold at Sat Jun  2 18:00:32 2018\n",
      "\n",
      "\n",
      "start  2 fold at  Sat Jun  2 18:00:32 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.56      0.58      0.57     21184\n",
      "          2       0.66      0.65      0.66     28330\n",
      "          3       0.84      0.74      0.79      3576\n",
      "          4       0.44      0.77      0.56       275\n",
      "          5       0.45      0.60      0.51       950\n",
      "          6       0.63      0.60      0.62      1737\n",
      "          7       0.85      0.71      0.77      2051\n",
      "\n",
      "avg / total       0.64      0.63      0.63     58103\n",
      "\n",
      "end  2 fold at Sat Jun  2 18:00:45 2018\n",
      "\n",
      "\n",
      "start  3 fold at  Sat Jun  2 18:00:45 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.48      0.50      0.49     21184\n",
      "          2       0.65      0.59      0.62     28330\n",
      "          3       0.83      0.74      0.78      3576\n",
      "          4       0.45      0.83      0.58       275\n",
      "          5       0.52      0.61      0.56       949\n",
      "          6       0.60      0.64      0.62      1737\n",
      "          7       0.51      0.89      0.65      2051\n",
      "\n",
      "avg / total       0.59      0.58      0.58     58102\n",
      "\n",
      "end  3 fold at Sat Jun  2 18:00:59 2018\n",
      "\n",
      "\n",
      "start  4 fold at  Sat Jun  2 18:00:59 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.43      0.52      0.47     21184\n",
      "          2       0.58      0.43      0.49     28330\n",
      "          3       0.83      0.75      0.79      3575\n",
      "          4       0.56      0.87      0.68       275\n",
      "          5       0.33      0.85      0.48       949\n",
      "          6       0.61      0.73      0.67      1737\n",
      "          7       0.54      0.86      0.67      2051\n",
      "\n",
      "avg / total       0.54      0.52      0.52     58101\n",
      "\n",
      "end  4 fold at Sat Jun  2 18:01:15 2018\n",
      "\n",
      "\n",
      "start  5 fold at  Sat Jun  2 18:01:15 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.52      0.52      0.52     21184\n",
      "          2       0.62      0.53      0.57     28330\n",
      "          3       0.76      0.74      0.75      3575\n",
      "          4       0.34      0.60      0.43       275\n",
      "          5       0.28      0.81      0.41       949\n",
      "          6       0.43      0.73      0.54      1737\n",
      "          7       0.52      0.74      0.61      2051\n",
      "\n",
      "avg / total       0.58      0.56      0.56     58101\n",
      "\n",
      "end  5 fold at Sat Jun  2 18:01:29 2018\n",
      "\n",
      "\n",
      "start  6 fold at  Sat Jun  2 18:01:29 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.58      0.62      0.60     21184\n",
      "          2       0.62      0.46      0.53     28330\n",
      "          3       0.46      0.68      0.55      3575\n",
      "          4       0.52      0.74      0.61       275\n",
      "          5       0.20      0.67      0.31       949\n",
      "          6       0.35      0.67      0.46      1737\n",
      "          7       0.68      0.78      0.73      2051\n",
      "\n",
      "avg / total       0.58      0.55      0.56     58101\n",
      "\n",
      "end  6 fold at Sat Jun  2 18:01:44 2018\n",
      "\n",
      "\n",
      "start  7 fold at  Sat Jun  2 18:01:44 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.52      0.54      0.53     21184\n",
      "          2       0.61      0.46      0.52     28330\n",
      "          3       0.69      0.69      0.69      3575\n",
      "          4       1.00      0.76      0.86       274\n",
      "          5       0.16      0.63      0.26       949\n",
      "          6       0.46      0.64      0.53      1736\n",
      "          7       0.42      0.95      0.59      2051\n",
      "\n",
      "avg / total       0.56      0.53      0.54     58099\n",
      "\n",
      "end  7 fold at Sat Jun  2 18:01:57 2018\n",
      "\n",
      "\n",
      "start  8 fold at  Sat Jun  2 18:01:57 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.51      0.65      0.57     21184\n",
      "          2       0.63      0.49      0.55     28330\n",
      "          3       0.87      0.68      0.76      3575\n",
      "          4       1.00      0.81      0.89       274\n",
      "          5       0.46      0.54      0.50       949\n",
      "          6       0.65      0.59      0.62      1736\n",
      "          7       0.66      0.97      0.79      2051\n",
      "\n",
      "avg / total       0.60      0.58      0.58     58099\n",
      "\n",
      "end  8 fold at Sat Jun  2 18:02:12 2018\n",
      "\n",
      "\n",
      "start  9 fold at  Sat Jun  2 18:02:12 2018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.69      0.80      0.74     21184\n",
      "          2       0.81      0.51      0.63     28330\n",
      "          3       0.44      0.86      0.58      3575\n",
      "          4       1.00      0.75      0.86       274\n",
      "          5       0.37      0.60      0.46       949\n",
      "          6       0.34      0.64      0.44      1736\n",
      "          7       0.55      0.94      0.69      2051\n",
      "\n",
      "avg / total       0.71      0.66      0.66     58099\n",
      "\n",
      "end  9 fold at Sat Jun  2 18:02:24 2018\n",
      "\n",
      "\n",
      "Extensive Analysis\n",
      "Average Accuracy:  0.580973965552\n",
      "Average Precision:  0.597243209973\n",
      "Average Recall:  0.689693707678\n",
      "Average fscore:  0.616373235349\n",
      "Average Confusion Matrix:\n",
      " [[ 11502.8   8553.3     22.5      0.     135.9     36.     933.5]\n",
      " [  9622.7  15803.5    705.8      8.    1449.7    559.9    180.5]\n",
      " [    10.2    232.1   2676.8    102.9     62.3    490.9      0.2]\n",
      " [     0.       1.2     42.4    209.6      0.      21.5      0. ]\n",
      " [    48.5    214.1     17.9      0.     660.1      8.1      0.6]\n",
      " [    17.4    170.1    319.5     29.1     12.4   1188.2      0. ]\n",
      " [   281.1     55.       0.       0.       0.6      0.    1714.3]]\n",
      "Total Time: 144.72353196144104\n"
     ]
    }
   ],
   "source": [
    "startProgramTime = time()\n",
    "print('start program time at', ctime(startProgramTime))\n",
    "\n",
    "#readdata\n",
    "rawdata = pd.read_csv('covtype.csv', header=None)\n",
    "rawdata = rawdata.values\n",
    "data = rawdata[:,0:54]\n",
    "label = rawdata[:,54]\n",
    "#processing_minmax\n",
    "min_max_scaler=preprocessing.MinMaxScaler()  \n",
    "standard_data=min_max_scaler.fit_transform(data)\n",
    "#create 10 fold and result variables\n",
    "ten_fold = StratifiedKFold(n_splits=10)\n",
    "ten_accuracy =  np.zeros(10)\n",
    "ten_precision = np.zeros(10)\n",
    "ten_recall = np.zeros(10)\n",
    "ten_f1 = np.zeros(10)\n",
    "ten_matrix = np.zeros(490).reshape(10,7,7)\n",
    "\n",
    "# accumulate the runtime of every fold\n",
    "operaTime = 0\n",
    "i = 0\n",
    "# construct the KNN classifier\n",
    "neighbours = KNeighborsClassifier(n_neighbors=1)\n",
    "#start algorithm\n",
    "for train, test in ten_fold.split(standard_data, label):\n",
    "    startTime = time()\n",
    "    print('start ', i, 'fold at ', ctime(startTime))\n",
    "    #PCA\n",
    "    pca = PCA(n_components=0.95,whiten=True)\n",
    "    train_x  = pca.fit_transform(standard_data[train])\n",
    "    test_x = pca.transform(standard_data[test])\n",
    "    #test result without pca\n",
    "    #train_x = standard_data[train]\n",
    "    #test_x = standard_data[test]\n",
    "    \n",
    "    \n",
    "    # set the KNN classifier\n",
    "    neighbours.fit(train_x, label[train])\n",
    "\n",
    "    # predict by KNN classifier\n",
    "    predicted = neighbours.predict(test_x)\n",
    "\n",
    "\n",
    "    # get accuracy value\n",
    "    accuracy = accuracy_score(label[test], predicted)\n",
    "\n",
    "    # get precision,recall,fscore,support value\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(label[test], predicted)\n",
    "\n",
    "    # print report for extensive analysis\n",
    "    print(classification_report(label[test], predicted))\n",
    "\n",
    "    # get average result based on the seven types\n",
    "    ten_accuracy[i] = np.average(accuracy)\n",
    "    ten_precision[i] = np.average(precision)\n",
    "    ten_recall[i] = np.average(recall)\n",
    "    ten_f1[i] = np.average(fscore)\n",
    "    ten_matrix[i] = confusion_matrix(label[test], predicted)\n",
    "    endTime = time()\n",
    "    print('end ', i, 'fold at', ctime(endTime))\n",
    "    print('\\n')\n",
    "    i += 1\n",
    "endProgramTime = time()\n",
    "print('Extensive Analysis')\n",
    "print('Average Accuracy: ', np.average(ten_accuracy))\n",
    "print('Average Precision: ', np.average(ten_precision))\n",
    "print('Average Recall: ', np.average(ten_recall))\n",
    "print('Average fscore: ', np.average(ten_f1))\n",
    "np.set_printoptions(suppress=True)\n",
    "print('Average Confusion Matrix:\\n',np.average(matrix,axis = 0))\n",
    "print('Total Time:', endProgramTime-startProgramTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
